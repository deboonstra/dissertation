---
title: "Simulations for `norm_glsx_mix_qic_corstr_sim.R`"
output:
    html_document:
        code_folding: hide
---

# System setup
```{r loading packages, results = FALSE, message = FALSE}
R <- list.files(path = "./simulations/R", pattern = "*.R", full.names = TRUE)
sapply(R, source, .GlobalEnv)
library(knitr)
library(kableExtra)
```

# Setting

## Data
Following Wang *et al.* (2012) with one-hundred replications, the correlated normal responses are generated from the model $$y_{ij} = \mathbf{X}_{ij}^{\top}\boldsymbol{\beta} + \varepsilon_{ij},$$ where $i = 1, \ldots, 200,$ $j = 1, \ldots, 4,$ $\mathbf{X}_{ij} = (x_{ij, 1}, \ldots, x_{ij, 6})^{\top},$ and $\boldsymbol{\beta} = (2.0, 3.0, 0.5, 0.0, 0.0, 0.0)^{\top}.$

- For the covariates,
    - $x_{ij,1}$ was generated from the Bernoulli(0.5) distribution, and 
    - each $x_{ij,k}$ for $k = 2 \ldots 6$ was generated from the multivariate normal distribution with mean 0 and an AR(1) covariance matrix with marginal variance 1 and auto-correlation coefficient 0.5.
- The random errors $(\varepsilon_{i1}, \ldots, \varepsilon_{i4})^{\top}$ are generated from the multivariate normal distribution with marginal mean 0, marginal variance 1, and an exchangeable correlation matrix with $\rho = 0.5$.

## Transformation of the response and covariate matrix
After the data was simulated, the response, $\mathbf{y},$ and the covariate matrix, $\mathbf{X},$ was transformed by:

1. fitting a fully saturated model with an unstructured working correlation matrix,
2. calculating $\mathbf{V}_{i} = \phi \mathbf{A}_{i}^{1/2}\mathbf{R}_{i}(\boldsymbol{\alpha})\mathbf{A}_{i}^{1/2}$ and finding $\mathbf{V}_{i}^{-1/2}$, and
3. obtaining $\mathbf{y}^{*} = \mathbf{V}_{i}^{-1/2}\mathbf{y}$ and $\mathbf{X}^{*} = \mathbf{V}_{i}^{-1/2}\mathbf{X}$.

## Best subsets
All of the $(2^6 - 1) \times 4 = 252$ subsets of the fully saturated model with the four correlation structures (independence, exchangeable, AR(1), and unstructured) being investigated were fit using either generalized linear models (GLMs)  or generalized estimating questions (GEEs). The estimating procedure used depends on the likelihood framework of interest. Then, for each simulated data set and likelihood framework, the mean and correlation structure pairing that resulted in the minimum information criterion (IC) value was selected as the best fitting model. The goodness-of-fit (GOF) term of the ICs in these simulations are obtained using the transformed response and covariate matrix while the penalty term is calculated using the original data. These results are different from `norm_glsx_bqicu_sim` because with those simulations the correlation structure was **not** selected. 

```{r}
nsims <- 100L
beta <- c(2.0, 3.0, 0.5, 0.0, 0.0, 0.0)
w <- which(beta != 0)
l <- length(w)
```

# Results

The simulation results are stored in:
```{r loading sim res}
load("./simulations/outputs/norm_glsx_mix_qic_corstr_sim/norm_glsx_mix_qic_corstr_sim.RData") 
```
and should be compared to `norm_bqicu_sim` to see the impact the transformation process has on the selection of the mean and correlation structures.

## IC across the simulations {.tabset .tabset-fade .tabset-pills}
Now for each of the models with the minimum IC, the criterion will be broken into its parts to see what is impacting the selection process. Additionally, this provides a look at the stability of the information criterion across the simulations. For these simulations, the goodness-of-fit (GOF) term was derived using either the likelihood or quasi-likelihood framework. When GLMs were used to fit the transformed response, the GOF was based on traditional likelihood. While when GEEs were used, the GOF was based on quasi-likelihood that assumes independence among the observations of the response. Unlike the GOF term, the penalty term of each IC was calculated using GEEs. The penalty is $2tr\left(\widehat{\boldsymbol{\Omega}}_{I}\widehat{\mathbf{V}}_{R}\right)$, where $\widehat{\boldsymbol{\Omega}}_{I}$ and $\widehat{\mathbf{V}}_{R}$ are estimated using the original data data. Remember that $2tr\left(\widehat{\boldsymbol{\Omega}}_{I}\widehat{\mathbf{V}}_{R}\right)$ is the penalty term used in QIC and CIC, and is needed when simulataneous selection of the mean and correlation structure is of interest. If selection of correlation structure is **not** needed, then QICu, AIC, or BIC could be used because the penalty term is focused on the number of parameters estimated.
```{r}
ic_quasi <- sapply(res_quasi, function(x) x$ic_min, simplify = TRUE)
ic_lik <- sapply(res_lik, function(x) x$ic_min, simplify = TRUE)

plot(
  x = seq_len(nsims), y = ic_quasi,
  ylim = c(600, 1000),
  xlab = "Index", ylab = "IC (quasi-likelihood)",
  type = "l", lwd = 2, col = "gray",
  bty = "n"
)
plot(
  x = seq_len(nsims), y = ic_lik,
  ylim = c(2000, 2400),
  xlab = "Index", ylab = "IC (likelihood)",
  type = "l", lwd = 2, col = "gray",
  bty = "n"
)
```

### Goodness-of-fit
```{r}
gf_quasi <- gf_lik <- rep(NA, nsims)
for (k in seq_len(nsims)) {
  w_quasi <- res_quasi[[k]]$min
  w_lik <- res_lik[[k]]$min
  gf_quasi[k] <- res_quasi[[k]]$gof[w_quasi]
  gf_lik[k] <- res_lik[[k]]$gof[w_lik]
}

plot(
  x = seq_len(nsims), y = gf_quasi,
  ylim = c(600, 1000),
  xlab = "Index", ylab = "GOF (quasi-likelihood)",
  type = "l", lwd = 2, col = "gray",
  bty = "n"
)

plot(
  x = seq_len(nsims), y = gf_lik,
  ylim = c(2000, 2400),
  xlab = "Index", ylab = "GOF (likelihood)",
  type = "l", lwd = 2, col = "gray",
  bty = "n"
)

plot(
  x = seq_len(nsims), y = gf_lik - gf_quasi,
  ylim = c(1460, 1480),
  xlab = "Index", ylab = "Difference of GOF (likelihood - quasi-likelihood)",
  type = "l", lwd = 2, col = "firebrick",
  bty = "n"
)
```

### Penalty
```{r}
pen_quasi <- pen_lik <-  rep(NA, nsims)
for (k in seq_len(nsims)) {
  w_quasi <- res_quasi[[k]]$min
  w_lik <- res_lik[[k]]$min
  pen_quasi[k] <- res_quasi[[k]]$penalty[w_quasi]
  pen_lik[k] <- res_lik[[k]]$penalty[w_lik]
}

plot(
  x = seq_len(nsims), y = pen_quasi,
  ylim = c(0, 15),
  xlab = "Index", ylab = "Penalty (quasi-likelihood)",
  type = "l", lwd = 2, col = "gray",
  bty = "n"
)

plot(
  x = seq_len(nsims), y = pen_lik,
  ylim = c(0, 15),
  xlab = "Index", ylab = "Penalty (likelihood)",
  type = "l", lwd = 2, col = "gray",
  bty = "n"
)
```

## Structure selection based on IC {.tabset .tabset-fade .tabset-pills}

### Mean
For measuring the overall performance of selecting the mean structure, a function will be created called `look` that reports the number of models, where

- the number of features selected is less than the number of non-zero coefficients in the generating model (`under`), 
- only the non-zero features from the generating model were selected (`exact`), 
- the number of features selected matches the number of non-zero coefficients in the generating model; however, at least one of the selected features is NOT one of the non-zero effects in the generating model (`mis`),
- the number of features selected is greated than the number of non-zero coefficients in the generating model (`over`), and
- the number of features selected is greated than the number of non-zero coefficients in the generating model and includes all the non-zero effects (`over_inc`).

```{r}
# creating a function to generate a look at the simulation results
look <- function(x) {
  under <- 0
  exact <- 0
  mis <- 0
  over <- 0
  over_inc <- 0
  for (k in seq_along(x)) {
    under <- under + (length(x[[k]]$vars) < l)
    exact <- exact + identical(x[[k]]$vars, w)
    mis <- mis + (!identical(x[[k]]$vars, w) & (length(x[[k]]$vars) == l))
    over <- over + (length(x[[k]]$vars) > l)
    over_inc <- over_inc + ((length(x[[k]]$vars) > l) & all(w %in% x[[k]]$vars))
  }
  return(
    data.frame(
      under = under, exact = exact,
      mis = mis, over = over,
      over_inc = over_inc
    )
  )
}
```

Using the `look` function, we have the following summary measures from the simulation set, where the models resulting in the minimum IC value were selected as the best fitting model and the counts are based on likelihood framework used to define the GOF term.

```{r}
data.frame(
    lik = c("Quasi-likelihood", "Likelihood"),
    dplyr::bind_rows(look(res_quasi), look(res_lik))
)
```

### Correlation
Now for measuring the overall performance of selecting the correlation structure, the table below presents the number of models, where the correlation structure selected from the best subsets was AR(1) (`ar1`), exchangeable (`exchangeable`), independence (`independence`), and unstructured (`un`). Note that proper selection of the mean structure is not needed to select the correlation structure correctly. Considering the within-subject random effects are generated using the exchangeable correlation structure, we would like to see that the exchangeable structured is selected the most.
```{r}
corstr_quasi <- sapply(res_quasi, function(x) x$corstr_min, simplify = TRUE)
corstr_lik <- sapply(res_lik, function(x) x$corstr_min, simplify = TRUE)

corstr_count <- function(x) {
  count_indp <- 0
  count_cs <- 0
  count_ar1 <- 0
  count_un <- 0
  for (i in seq_along(x)) {
    count_indp <- count_indp + (x[i] == "independence")
    count_cs <- count_cs + (x[i] == "exchangeable")
    count_ar1 <- count_ar1 + (x[i] == "ar1")
    count_un <- count_un + (x[i] == "unstructured")
  }
  temp <- data.frame(
    indpendenence = count_indp, exchangeable = count_cs,
    ar1 = count_ar1, un = count_un
  )
  return(temp)
}

data.frame(
  lik = c("Quasi-likelihood", "Likelihood"),
  dplyr::bind_rows(corstr_count(corstr_quasi), corstr_count(corstr_lik))
)
```

Based on these results, it appears that the unstructured correlation structure was selected the majority of time regardless of likelihood framework used to define the goodness-of-fit term.

### Joint
Thus, on average, the models, with the minimum information criteria, have mean structures that includes all the non-zero coefficients at minimum and an improper correlation structure.
```{r}
joint <- function(x) {
  p <- function(y) {
    if (length(y) == 0) {
      return(data.frame(under = 0, exact = 0, mis = 0, over = 0, over_inc = 0))
    } else {
      return(look(x = y))
    }
  }

  independence <- x[sapply(x, function(x) x$corstr_min == "independence", simplify = TRUE)]
  exchangeable <- x[sapply(x, function(x) x$corstr_min == "exchangeable", simplify = TRUE)]
  ar1 <- x[sapply(x, function(x) x$corstr_min == "ar1", simplify = TRUE)]
  un <- x[sapply(x, function(x) x$corstr_min == "unstructured", simplify = TRUE)]

  ip <- p(independence)
  ep <- p(exchangeable)
  ap <- p(ar1)
  up <- p(un)

  return(dplyr::bind_rows(ip, ep, ap, up))
}

data.frame(
  lik = c("Quasi-likelihood", "", "", "", "Likelihood", "", "", ""),
  corstr = rep(c("Independence", "Exchangeable", "AR(1)", "Unstructured"), 2),
  dplyr::bind_rows(joint(res_quasi), joint(res_lik))
)
```